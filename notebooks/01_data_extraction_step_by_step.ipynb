{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789e16ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-16T08:20:45.816606Z",
     "start_time": "2022-06-16T08:20:45.001024Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0dbcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_files(df, path, file_name = 'test_details.csv', details=True, summary=True, cycles=False):\n",
    "    '''creation of the csv files with headers'''\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if details:\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        with open(file_path, 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=one_val_cols_list)\n",
    "            writer.writeheader()\n",
    "        print(f\"{file_name} created\")\n",
    "    \n",
    "    if summary:\n",
    "        print(\"initializing summary files\")\n",
    "        for values in df.index:\n",
    "            file_path = os.path.join(path, f\"summary_{values}.csv\")\n",
    "            with open(file_path, 'w') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=['barcode'] +[i for i in range(0,3000)])\n",
    "                writer.writeheader()\n",
    "        print(\"summary_****.csv files initialized\")\n",
    "\n",
    "    if cycles:\n",
    "        for values in df.index:\n",
    "            file_path = os.path.join(path, f\"cycles_interpolated_{values}.csv\")\n",
    "            with open(file_path, 'w') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=['barcode'] +[i for i in range(0,2_000_000)])\n",
    "                writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "301b97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valide_shape(df,indexes,cols,one_val_cols,NaN_cols):\n",
    "    '''\n",
    "    validate that i) each df has the same shape as the 1st df opened\n",
    "    and that ii) each df has nan or unique values in specific columns\n",
    "    '''\n",
    "    #the new df has the same shape as the 1st one\n",
    "    assert(df.index.all() == indexes.all())\n",
    "    assert(df.columns.all() == cols.all())\n",
    "\n",
    "    #unique values and nan columns are identified\n",
    "    for col in one_val_cols:\n",
    "        assert(len(set(df[col].values))== 1)\n",
    "    for col in NaN_cols:\n",
    "        assert(df[col].isna().sum()== 21)\n",
    "        \n",
    "    print('file shape is valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048979b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lines_details(df,path, file_name = 'test_details.csv'):\n",
    "    '''add a line on the csv file_details for each data file and save the barcodes / file names relations'''\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    \n",
    "    dict = {}\n",
    "    #the only values added in this file are the one that are constant for a given battery\n",
    "    #they are parameters of the model\n",
    "    for val in one_val_cols_list:\n",
    "        dict[val] = df[val].iloc[0]\n",
    "\n",
    "    with open(file_path, 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=dict.keys())\n",
    "        writer.writerow(dict)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657ec741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lines_data(barcode, file_names,path_input,path_output, summary=True, cycles=False):\n",
    "    '''\n",
    "    Add the data for a given battery (identified by its barcode) to the csv file\n",
    "    File_names contain one or two names (some battery data are spread in two consecutive files)\n",
    "    The method merge the data if there are two files\n",
    "    '''\n",
    "    \n",
    "    dict_df = {}\n",
    "    for file_name in file_names:\n",
    "        #create a dict containing the df corresponding to barcode (one or two)\n",
    "\n",
    "        file_path = os.path.join(path_input, file_name)\n",
    "        dict_df[file_name] = pd.read_json(file_path)\n",
    "    \n",
    "    #creation of a variable 'values' containing all the measurement types\n",
    "    values = dict_df[file_names[0]].index\n",
    "\n",
    "    if summary:\n",
    "        #for each measurement\n",
    "        for value in values:\n",
    "            #init an empty list\n",
    "            data_list = []\n",
    "            for df in dict_df.values():\n",
    "                #if the data is not a 0.0 (empty)\n",
    "                if isinstance(df['summary'][value], float) == False:\n",
    "                    #the data is added to data list (one or two times)\n",
    "                    data_list = df['summary'][value] + data_list\n",
    "            #adding the barcode at the beginning of the file\n",
    "            data_list = [barcode] + data_list\n",
    "            \n",
    "            #add a row in the csv file\n",
    "            file_path = os.path.join(path_output, f\"summary_{value}.csv\")\n",
    "            with open(file_path, 'a') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(data_list)\n",
    "    \n",
    "    #same method as above but for 'cycles' files\n",
    "    #+ data compression to limit the size od the output csv files\n",
    "    if cycles:\n",
    "        for value in values:\n",
    "            data_list = []\n",
    "            for key, df in dict_df.items():\n",
    "                if isinstance(df['cycles_interpolated'][value], float) == False:\n",
    "                    nl =  np.array(df['cycles_interpolated'][value])\n",
    "                    try:\n",
    "                        nl = nl.astype('float32')\n",
    "                        print('reduction done')\n",
    "                    except:\n",
    "                        pass\n",
    "                    nl = list(nl)\n",
    "                    data_list = nl + data_list\n",
    "            data_list = [barcode] + data_list\n",
    "\n",
    "            file_path = os.path.join(path_output, f\"cycles_interpolated_{value}.csv\")\n",
    "            with open(file_path, 'a') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf396452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_protocol_string(protocol):\n",
    "    \"\"\" protocol extraction string \"\"\"\n",
    "    res = pd.DataFrame()\n",
    "    #protocol = protocol.lower().encode('unicode_escape').decode()\n",
    "    protocol = protocol.lower()\n",
    "    tmp = protocol.split(\"\\\\\")\n",
    "    tmp1 = (tmp[1].split(\"-\")[1]).split(\"c\")\n",
    "\n",
    "    batch = tmp[0]\n",
    "    c1 = tmp1[0].split(\"c\")[0]\n",
    "    per = protocol.split(\"per\")[0].split(\"_\")[-1].split('-')[-1]\n",
    "    c2 = protocol.split(\"per_\")[1].split(\"c\")[0]\n",
    "    newstructure = int(protocol.find(\"newstructure\") >= 0)\n",
    "    \n",
    "    return batch, c1, per, c2, newstructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3045c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_test_details(path, old_name = 'test_details.csv', new_name = 'test_details_clean.csv'):\n",
    "    input_path = os.path.join(path, old_name)\n",
    "    output_path = os.path.join(path, new_name)\n",
    "    df = pd.read_csv(input_path)\n",
    "    df['batch'], df['c1'], df['per'], df['c2'], df['newstructure'] = zip(*df['protocol'].map(extract_protocol_string))\n",
    "    df = df.drop(columns = 'protocol')\n",
    "    df.to_csv(output_path)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "833825a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params, if require to change folder names\n",
    "initial_data_folder_name = 'InitialData'\n",
    "transformed_data_folder_name = 'TransformedData'\n",
    "get_details = True\n",
    "get_summary = True\n",
    "get_cycles_interpolated = False\n",
    "\n",
    "#list of barcode to drop (this information is given by the paper)\n",
    "bc_to_drop = [\n",
    "'EL150800465027',\n",
    "'EL150800464002',\n",
    "'EL150800463980',\n",
    "'EL150800463882',\n",
    "'EL150800460653',\n",
    "'EL150800460596',\n",
    "'EL150800460518',\n",
    "'EL150800460605',\n",
    "'EL150800460451',\n",
    "'EL150800460478',\n",
    "'EL150800737234',\n",
    "'EL150800737380',\n",
    "'EL150800737386',\n",
    "'EL150800737299',\n",
    "'EL150800737350',\n",
    "'EL150800739477']\n",
    "\n",
    "#list of unique values (added into the test_details file)\n",
    "one_val_cols_list = ['@module', '@class', 'barcode', 'protocol', 'channel_id', '@version']\n",
    "\n",
    "#columns not used during sampling\n",
    "NaN_cols_list = ['diagnostic_summary', 'diagnostic_interpolated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa5082bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(initial_data_folder_name,\n",
    "                   transformed_data_folder_name,\n",
    "                   details,\n",
    "                   summary,\n",
    "                   cylces,\n",
    "                    barcode_to_drop):\n",
    "    '''\n",
    "    This method transform raw data from the paper (several JSON files, with one JSON corresponding to one battery)\n",
    "    The output is one file correspond to one type of measurement and caontains the data of\n",
    "    all the batteries for this given measurement\n",
    "    The method also create a 'test_details file containing all the features of the batteries\n",
    "    and important information regarding protocols\n",
    "    The method can transform both 'summary' data and 'interpolated data'. However 'interpolated'\n",
    "    are too large, therefore they are not used in this project \n",
    "    '''\n",
    "    #from a python file, automatically get the current directory\n",
    "    #dir_path = os.path.dirname(__file__)\n",
    "\n",
    "    #also possible to manualy give the current directory to the method\n",
    "    dir_path = '/home/romainj/code/RomainJupille/wagon/Projet_batteries/BatteryProject/BatteryProject'\n",
    "\n",
    "\n",
    "    #creation of paths towards initial folder and final folder\n",
    "    initial_data_path = os.path.join(dir_path, \"..\", \"..\", initial_data_folder_name)\n",
    "    initial_data_path = os.path.normpath(initial_data_path)\n",
    "    transformed_data_path = os.path.join(dir_path, \"..\", \"..\", transformed_data_folder_name)\n",
    "    transformed_data_path = os.path.normpath(transformed_data_path)\n",
    "\n",
    "    # files names to transform (from initial folder)\n",
    "    json_files = [f for f in os.listdir(initial_data_path) if f[-4:] == 'json']\n",
    "\n",
    "    # definition of the shape of the 1st file\n",
    "    first_file_path = os.path.join(initial_data_path, json_files[0])\n",
    "    df = pd.read_json(first_file_path)\n",
    "    #get indexes and columns of the data of the 1st battery\n",
    "    indexes_list = df.index\n",
    "    column_list = df.columns\n",
    "\n",
    "    #initialization of the csv files in the output folder, based on columns and indexes of the 1st file\n",
    "    #options\n",
    "    initialize_files(df, transformed_data_path, details=get_details, summary=get_summary, cycles=get_cycles_interpolated )\n",
    "\n",
    "\n",
    "    '''\n",
    "    for all the JSON files (one json corresponding to one battery)\n",
    "    '''\n",
    "    # values that will be dropped during processing\n",
    "    one_val_cols_list = ['@module', '@class', 'barcode', 'protocol', 'channel_id', '@version']\n",
    "    NaN_cols_list = ['diagnostic_summary', 'diagnostic_interpolated']\n",
    "    file_droped = 0\n",
    "    file_concat = 0\n",
    "    files_dict = {}\n",
    "\n",
    "    i = 0\n",
    "    for file_name in json_files:\n",
    "        #dowloading data into a dataframe\n",
    "        file_path = os.path.join(initial_data_path, file_name)\n",
    "        df = pd.read_json(file_path)\n",
    "\n",
    "        #validation of the shape od the data that will be processed\n",
    "        valide_shape(df,indexes_list,column_list,one_val_cols_list,NaN_cols_list)\n",
    "\n",
    "        #get the barcode of the file\n",
    "        bc = df['barcode'].iloc[0]\n",
    "\n",
    "        #check if the data has to be dropped (some samples had issues during data acquisition)\n",
    "        if bc.upper() in barcode_to_drop:\n",
    "            file_droped += 1\n",
    "            print(f\"{bc} has been dropped\")\n",
    "\n",
    "        #if not add the file into the dict of bc (check for duplicated barcode)\n",
    "        #One barcode can be linked to 1 or 2 files (some battery measurement extended over 2 periods)\n",
    "        else:\n",
    "            if bc in files_dict.keys():\n",
    "                files_dict[bc].append(file_name)\n",
    "                file_concat += 1\n",
    "            else:\n",
    "                files_dict[bc] = [file_name]\n",
    "                add_lines_details(df,transformed_data_path)\n",
    "\n",
    "        i += 1\n",
    "        if i%10 ==0:\n",
    "            print(f\"{i} files checked \")\n",
    "\n",
    "    clean_test_details(transformed_data_path)\n",
    "\n",
    "    print('All files have been checked and the test_details file has been created')\n",
    "    print(f\"{file_droped} files droped\")\n",
    "    print(f\"{file_concat} files concatenated\")\n",
    "\n",
    "\n",
    "    i=0\n",
    "    #Finally, for each barcode saved previously, data are added into the csv files\n",
    "    for barcode, file_names in files_dict.items():\n",
    "        \n",
    "        add_lines_data(barcode, file_names,initial_data_path,transformed_data_path, summary=get_summary, cycles=get_cycles_interpolated )\n",
    "        print(f\"Barcodes{i} read and the data has been added to the csv files\")\n",
    "        i+=1\n",
    "\n",
    "    print(f\"{i} lines created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab1af994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FastCharge_000069_CH25_structure.json', 'FastCharge_000002_CH34_structure.json', 'FastCharge_000007_CH24_structure.json', 'FastCharge_000065_CH34_structure.json', 'FastCharge_000004_CH1_structure.json', 'FastCharge_000036_CH48_structure.json', 'FastCharge_000032_CH22_structure.json', 'FastCharge_000004_CH3_structure.json', 'FastCharge_000026_CH5_structure.json', 'FastCharge_000020_CH23_structure.json', 'FastCharge_000024_CH45_structure.json', 'FastCharge_000052_CH41_structure.json', 'FastCharge_000025_CH7_structure.json', 'FastCharge_000008_CH48_structure.json', 'FastCharge_000002_CH26_structure.json', 'FastCharge_000016_CH17_structure.json', 'FastCharge_000066_CH26_structure.json', 'FastCharge_000012_CH21_structure.json', 'FastCharge_000034_CH27_structure.json', 'FastCharge_000008_CH47_structure.json', 'FastCharge_000002_CH2_structure.json', 'FastCharge_000045_CH1_structure.json', 'FastCharge_000018_CH18_structure.json', 'FastCharge_000033_CH14_structure.json', 'FastCharge_000012_CH5_structure.json', 'FastCharge_000048_CH44_structure.json', 'FastCharge_000003_CH40_structure.json', 'FastCharge_000001_CH16_structure.json', 'FastCharge_000042_CH36_structure.json', 'FastCharge_000003_CH39_structure.json', 'FastCharge_000011_CH26_structure.json', 'FastCharge_000039_CH28_structure.json', 'FastCharge_000059_CH33_structure.json', 'FastCharge_000017_CH33_structure.json', 'FastCharge_000064_CH39_structure.json', 'FastCharge_000023_CH38_structure.json', 'FastCharge_000015_CH14_structure.json', 'FastCharge_000017_CH25_structure.json', 'FastCharge_000015_CH12_structure.json', 'FastCharge_000027_CH31_structure.json', 'FastCharge_000057_CH36_structure.json', 'FastCharge_000058_CH46_structure.json', 'FastCharge_000009_CH4_structure.json', 'FastCharge_000017_CH17_structure.json', 'FastCharge_000041_CH10_structure.json', 'FastCharge_000019_CH30_structure.json', 'FastCharge_000029_CH10_structure.json', 'FastCharge_000002_CH42_structure.json', 'FastCharge_000006_CH3_structure.json', 'FastCharge_000014_CH24_structure.json', 'FastCharge_000039_CH27_structure.json', 'FastCharge_000002_CH10_structure.json', 'FastCharge_000006_CH11_structure.json', 'FastCharge_000065_CH33_structure.json', 'FastCharge_000001_CH38_structure.json', 'FastCharge_000029_CH9_structure.json', 'FastCharge_000009_CH8_structure.json', 'FastCharge_000013_CH13_structure.json', 'FastCharge_000015_CH20_structure.json', 'FastCharge_000026_CH6_structure.json', 'FastCharge_000015_CH22_structure.json', 'FastCharge_000015_CH4_structure.json', 'FastCharge_000012_CH23_structure.json', 'FastCharge_000044_CH12_structure.json', 'FastCharge_000050_CH32_structure.json', 'FastCharge_000006_CH35_structure.json', 'FastCharge_000015_CH36_structure.json', 'FastCharge_000035_CH12_structure.json', 'FastCharge_000037_CH21_structure.json', 'FastCharge_000051_CH16_structure.json', 'FastCharge_000049_CH37_structure.json', 'FastCharge_000030_CH30_structure.json', 'FastCharge_000053_CH42_structure.json', 'FastCharge_000022_CH24_structure.json', 'FastCharge_000063_CH5_structure.json', 'FastCharge_000061_CH43_structure.json', 'FastCharge_000070_CH46_structure.json', 'FastCharge_000046_CH32_structure.json', 'FastCharge_000067_CH41_structure.json', 'FastCharge_000019_CH29_structure.json', 'FastCharge_000043_CH16_structure.json', 'FastCharge_000050_CH40_structure.json', 'FastCharge_000004_CH2_structure.json', 'FastCharge_000006_CH27_structure.json', 'FastCharge_000010_CH44_structure.json', 'FastCharge_000009_CH7_structure.json', 'FastCharge_000002_CH47_structure.json', 'FastCharge_000021_CH28_structure.json', 'FastCharge_000005_CH18_structure.json', 'FastCharge_000051_CH15_structure.json', 'FastCharge_000071_CH40_structure.json', 'FastCharge_000045_CH3_structure.json', 'FastCharge_000037_CH22_structure.json', 'FastCharge_000002_CH18_structure.json', 'FastCharge_000006_CH8_structure.json', 'FastCharge_000068_CH38_structure.json', 'FastCharge_000067_CH42_structure.json', 'FastCharge_000066_CH25_structure.json', 'FastCharge_000017_CH41_structure.json', 'FastCharge_000012_CH45_structure.json', 'FastCharge_000062_CH47_structure.json', 'FastCharge_000012_CH13_structure.json', 'FastCharge_000031_CH9_structure.json', 'FastCharge_000006_CH48_structure.json', 'FastCharge_000002_CH7_structure.json', 'FastCharge_000058_CH45_structure.json', 'FastCharge_000028_CH20_structure.json', 'FastCharge_000063_CH6_structure.json', 'FastCharge_000054_CH15_structure.json', 'FastCharge_000023_CH37_structure.json', 'FastCharge_000017_CH1_structure.json', 'FastCharge_000047_CH21_structure.json', 'FastCharge_000015_CH44_structure.json', 'FastCharge_000048_CH43_structure.json', 'FastCharge_000046_CH31_structure.json', 'FastCharge_000028_CH19_structure.json', 'FastCharge_000012_CH29_structure.json', 'FastCharge_000013_CH14_structure.json', 'FastCharge_000007_CH39_structure.json', 'FastCharge_000056_CH11_structure.json', 'FastCharge_000017_CH46_structure.json', 'FastCharge_000073_CH35_structure.json', 'FastCharge_000057_CH35_structure.json', 'FastCharge_000017_CH9_structure.json', 'FastCharge_000014_CH23_structure.json', 'FastCharge_000040_CH32_structure.json', 'FastCharge_000045_CH2_structure.json', 'FastCharge_000006_CH43_structure.json', 'FastCharge_000072_CH34_structure.json', 'FastCharge_000012_CH37_structure.json', 'FastCharge_000000_CH19_structure.json', 'FastCharge_000017_CH6_structure.json', 'FastCharge_000044_CH11_structure.json', 'FastCharge_000015_CH28_structure.json', 'FastCharge_000001_CH30_structure.json', 'FastCharge_000060_CH29_structure.json', 'FastCharge_000038_CH13_structure.json', 'FastCharge_000055_CH20_structure.json', 'FastCharge_000012_CH15_structure.json', 'FastCharge_000006_CH19_structure.json']\n",
      "test_details.csv created\n",
      "initializing summary files\n",
      "summary_****.csv files initialized\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "el150800737386 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "10 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "el150800460478 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "20 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "30 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "el150800737299 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "40 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "el150800460605 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "EL150800737380 has been dropped\n",
      "file shape is valid\n",
      "50 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "el150800737234 has been dropped\n",
      "file shape is valid\n",
      "EL150800465027 has been dropped\n",
      "file shape is valid\n",
      "el150800460596 has been dropped\n",
      "file shape is valid\n",
      "EL150800463980 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "60 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "70 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "80 files checked \n",
      "file shape is valid\n",
      "el150800460451 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "90 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "100 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "el150800460518 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "110 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "el150800739477 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "EL150800463882 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "120 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "EL150800460653 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "el150800737350 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "130 files checked \n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "EL150800464002 has been dropped\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "file shape is valid\n",
      "140 files checked \n",
      "All files have been checked and the test_details file has been created\n",
      "16 files droped\n",
      "5 files concatenated\n",
      "Barcodes0 read and the data has been added to the csv files\n",
      "Barcodes1 read and the data has been added to the csv files\n",
      "Barcodes2 read and the data has been added to the csv files\n",
      "Barcodes3 read and the data has been added to the csv files\n",
      "Barcodes4 read and the data has been added to the csv files\n",
      "Barcodes5 read and the data has been added to the csv files\n",
      "Barcodes6 read and the data has been added to the csv files\n",
      "Barcodes7 read and the data has been added to the csv files\n",
      "Barcodes8 read and the data has been added to the csv files\n",
      "Barcodes9 read and the data has been added to the csv files\n",
      "Barcodes10 read and the data has been added to the csv files\n",
      "Barcodes11 read and the data has been added to the csv files\n",
      "Barcodes12 read and the data has been added to the csv files\n",
      "Barcodes13 read and the data has been added to the csv files\n",
      "Barcodes14 read and the data has been added to the csv files\n",
      "Barcodes15 read and the data has been added to the csv files\n",
      "Barcodes16 read and the data has been added to the csv files\n",
      "Barcodes17 read and the data has been added to the csv files\n",
      "Barcodes18 read and the data has been added to the csv files\n",
      "Barcodes19 read and the data has been added to the csv files\n",
      "Barcodes20 read and the data has been added to the csv files\n",
      "Barcodes21 read and the data has been added to the csv files\n",
      "Barcodes22 read and the data has been added to the csv files\n",
      "Barcodes23 read and the data has been added to the csv files\n",
      "Barcodes24 read and the data has been added to the csv files\n",
      "Barcodes25 read and the data has been added to the csv files\n",
      "Barcodes26 read and the data has been added to the csv files\n",
      "Barcodes27 read and the data has been added to the csv files\n",
      "Barcodes28 read and the data has been added to the csv files\n",
      "Barcodes29 read and the data has been added to the csv files\n",
      "Barcodes30 read and the data has been added to the csv files\n",
      "Barcodes31 read and the data has been added to the csv files\n",
      "Barcodes32 read and the data has been added to the csv files\n",
      "Barcodes33 read and the data has been added to the csv files\n",
      "Barcodes34 read and the data has been added to the csv files\n",
      "Barcodes35 read and the data has been added to the csv files\n",
      "Barcodes36 read and the data has been added to the csv files\n",
      "Barcodes37 read and the data has been added to the csv files\n",
      "Barcodes38 read and the data has been added to the csv files\n",
      "Barcodes39 read and the data has been added to the csv files\n",
      "Barcodes40 read and the data has been added to the csv files\n",
      "Barcodes41 read and the data has been added to the csv files\n",
      "Barcodes42 read and the data has been added to the csv files\n",
      "Barcodes43 read and the data has been added to the csv files\n",
      "Barcodes44 read and the data has been added to the csv files\n",
      "Barcodes45 read and the data has been added to the csv files\n",
      "Barcodes46 read and the data has been added to the csv files\n",
      "Barcodes47 read and the data has been added to the csv files\n",
      "Barcodes48 read and the data has been added to the csv files\n",
      "Barcodes49 read and the data has been added to the csv files\n",
      "Barcodes50 read and the data has been added to the csv files\n",
      "Barcodes51 read and the data has been added to the csv files\n",
      "Barcodes52 read and the data has been added to the csv files\n",
      "Barcodes53 read and the data has been added to the csv files\n",
      "Barcodes54 read and the data has been added to the csv files\n",
      "Barcodes55 read and the data has been added to the csv files\n",
      "Barcodes56 read and the data has been added to the csv files\n",
      "Barcodes57 read and the data has been added to the csv files\n",
      "Barcodes58 read and the data has been added to the csv files\n",
      "Barcodes59 read and the data has been added to the csv files\n",
      "Barcodes60 read and the data has been added to the csv files\n",
      "Barcodes61 read and the data has been added to the csv files\n",
      "Barcodes62 read and the data has been added to the csv files\n",
      "Barcodes63 read and the data has been added to the csv files\n",
      "Barcodes64 read and the data has been added to the csv files\n",
      "Barcodes65 read and the data has been added to the csv files\n",
      "Barcodes66 read and the data has been added to the csv files\n",
      "Barcodes67 read and the data has been added to the csv files\n",
      "Barcodes68 read and the data has been added to the csv files\n",
      "Barcodes69 read and the data has been added to the csv files\n",
      "Barcodes70 read and the data has been added to the csv files\n",
      "Barcodes71 read and the data has been added to the csv files\n",
      "Barcodes72 read and the data has been added to the csv files\n",
      "Barcodes73 read and the data has been added to the csv files\n",
      "Barcodes74 read and the data has been added to the csv files\n",
      "Barcodes75 read and the data has been added to the csv files\n",
      "Barcodes76 read and the data has been added to the csv files\n",
      "Barcodes77 read and the data has been added to the csv files\n",
      "Barcodes78 read and the data has been added to the csv files\n",
      "Barcodes79 read and the data has been added to the csv files\n",
      "Barcodes80 read and the data has been added to the csv files\n",
      "Barcodes81 read and the data has been added to the csv files\n",
      "Barcodes82 read and the data has been added to the csv files\n",
      "Barcodes83 read and the data has been added to the csv files\n",
      "Barcodes84 read and the data has been added to the csv files\n",
      "Barcodes85 read and the data has been added to the csv files\n",
      "Barcodes86 read and the data has been added to the csv files\n",
      "Barcodes87 read and the data has been added to the csv files\n",
      "Barcodes88 read and the data has been added to the csv files\n",
      "Barcodes89 read and the data has been added to the csv files\n",
      "Barcodes90 read and the data has been added to the csv files\n",
      "Barcodes91 read and the data has been added to the csv files\n",
      "Barcodes92 read and the data has been added to the csv files\n",
      "Barcodes93 read and the data has been added to the csv files\n",
      "Barcodes94 read and the data has been added to the csv files\n",
      "Barcodes95 read and the data has been added to the csv files\n",
      "Barcodes96 read and the data has been added to the csv files\n",
      "Barcodes97 read and the data has been added to the csv files\n",
      "Barcodes98 read and the data has been added to the csv files\n",
      "Barcodes99 read and the data has been added to the csv files\n",
      "Barcodes100 read and the data has been added to the csv files\n",
      "Barcodes101 read and the data has been added to the csv files\n",
      "Barcodes102 read and the data has been added to the csv files\n",
      "Barcodes103 read and the data has been added to the csv files\n",
      "Barcodes104 read and the data has been added to the csv files\n",
      "Barcodes105 read and the data has been added to the csv files\n",
      "Barcodes106 read and the data has been added to the csv files\n",
      "Barcodes107 read and the data has been added to the csv files\n",
      "Barcodes108 read and the data has been added to the csv files\n",
      "Barcodes109 read and the data has been added to the csv files\n",
      "Barcodes110 read and the data has been added to the csv files\n",
      "Barcodes111 read and the data has been added to the csv files\n",
      "Barcodes112 read and the data has been added to the csv files\n",
      "Barcodes113 read and the data has been added to the csv files\n",
      "Barcodes114 read and the data has been added to the csv files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barcodes115 read and the data has been added to the csv files\n",
      "Barcodes116 read and the data has been added to the csv files\n",
      "Barcodes117 read and the data has been added to the csv files\n",
      "Barcodes118 read and the data has been added to the csv files\n",
      "119 lines created\n"
     ]
    }
   ],
   "source": [
    "transform_data(initial_data_folder_name, transformed_data_folder_name, get_details, get_summary, get_cycles_interpolated, bc_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad715d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
