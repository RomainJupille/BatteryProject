{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789e16ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-16T08:20:45.816606Z",
     "start_time": "2022-06-16T08:20:45.001024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPOUR COMMENTAIRE :\\nmauvaises cellules batch 1\\n['EL150800465027',1]\\n['EL150800464002',1]\\n['EL150800463980',1]\\n['EL150800463882',1]\\n['EL150800460653',1]\\nmauvaises cellules batch 2\\n['el150800460596',2]\\n['el150800460518',2]\\n['el150800460605',2]\\n['el150800460451',2]\\n['el150800460478',2]\\nmauvaises cellules batch 3\\n['el150800737234',3]\\n['EL150800737380',3]\\n['el150800737386',3]\\n['el150800737299',3]\\n['el150800737350',3]\\n['el150800739477',3]\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "one_val_cols_list = ['@module', '@class', 'barcode', 'protocol', 'channel_id', '@version']\n",
    "NaN_cols_list = ['diagnostic_summary', 'diagnostic_interpolated']\n",
    "\n",
    "#list of barcode to drop (this information is given by the paper)\n",
    "barcode_to_drop = []\n",
    "\n",
    "\"\"\"\n",
    "POUR COMMENTAIRE :\n",
    "mauvaises cellules batch 1\n",
    "['EL150800465027',1]\n",
    "['EL150800464002',1]\n",
    "['EL150800463980',1]\n",
    "['EL150800463882',1]\n",
    "['EL150800460653',1]\n",
    "mauvaises cellules batch 2\n",
    "['el150800460596',2]\n",
    "['el150800460518',2]\n",
    "['el150800460605',2]\n",
    "['el150800460451',2]\n",
    "['el150800460478',2]\n",
    "mauvaises cellules batch 3\n",
    "['el150800737234',3]\n",
    "['EL150800737380',3]\n",
    "['el150800737386',3]\n",
    "['el150800737299',3]\n",
    "['el150800737350',3]\n",
    "['el150800739477',3]\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fe0c975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-16T08:55:38.699106Z",
     "start_time": "2022-06-16T08:55:37.612340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files have been added in the dictionnary\n",
      "0\n",
      "0\n",
      "0\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def initialize_files(df, path, details=True, summary=True, cycles=True, cycles_split=False):\n",
    "    '''creation of the csv files with headers'''\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if details:\n",
    "        file_path = os.path.join(path, 'test_details.csv')\n",
    "        with open(file_path, 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=one_val_cols_list)\n",
    "            writer.writeheader()\n",
    "    \n",
    "    if summary:\n",
    "        print(\"initialize_files, summary loop\")\n",
    "        for values in df.index:\n",
    "            file_path = os.path.join(path, f\"summary_{values}.csv\")\n",
    "            with open(file_path, 'w') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=['barcode'] +[i for i in range(0,3000)])\n",
    "                writer.writeheader()\n",
    "\n",
    "    if cycles:\n",
    "        for values in df.index:\n",
    "            file_path = os.path.join(path, f\"cycles_interpolated_{values}.csv\")\n",
    "            with open(file_path, 'w') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=['barcode'] +[i for i in range(0,2_000_000)])\n",
    "                writer.writeheader()\n",
    "    \n",
    "    if cycles_split:\n",
    "        for values in df.index:\n",
    "            folder_path = os.path.join(path, f\"cycles_interpolated_{values}\")\n",
    "            Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "def valide_shape(df,indexes,cols,one_val_cols,NaN_cols):\n",
    "    '''validate that the file has the same shape as the 1st file opened'''\n",
    "    assert(df.index.all() == indexes.all())\n",
    "    assert(df.columns.all() == cols.all())\n",
    "    for col in one_val_cols:\n",
    "        assert(len(set(df[col].values))== 1)\n",
    "    for col in NaN_cols:\n",
    "        assert(df[col].isna().sum()== 21)\n",
    "        \n",
    "        \n",
    "def add_lines_details(df,path):\n",
    "    '''add a line on the csv file_details for each data file and save the barcodes / file names relations in a dict'''\n",
    "    file_path = os.path.join(path, 'test_details.csv')\n",
    "\n",
    "    dict_ = {}\n",
    "    for val in one_val_cols_list:\n",
    "        dict_[val] = df[val].iloc[0]\n",
    "\n",
    "    with open(file_path, 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=dict_.keys())\n",
    "        writer.writerow(dict_)\n",
    "        \n",
    "    print(\"dict:\", dict_)\n",
    "\n",
    "    \n",
    "def add_lines_data(barcode, file_names,path_input,path_output, details=True, summary=True, cycles=True, cycles_split=False):\n",
    "    '''\n",
    "    Add data to all csv files\n",
    "    The method manages the duplicate codebars\n",
    "    '''\n",
    "    dict_df = {}\n",
    "    for file_name in file_names:\n",
    "        #create a dict containing all the df corresponding to barcode\n",
    "        print(file_name)\n",
    "\n",
    "        file_path = os.path.join(path_input, file_name)\n",
    "        dict_df[file_name] = pd.read_json(file_path)\n",
    "\n",
    "    values = dict_df[file_names[0]].index\n",
    "\n",
    "    if summary:\n",
    "        for value in values:\n",
    "            data_list = []\n",
    "            for key, df in dict_df.items():\n",
    "                if isinstance(df['summary'][value], float) == False:\n",
    "                    data_list = df['summary'][value] + data_list\n",
    "            data_list = [barcode] + data_list\n",
    "\n",
    "            file_path = os.path.join(path_output, f\"summary_{value}.csv\")\n",
    "            with open(file_path, 'a') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(data_list)\n",
    "                \n",
    "    if cycles:\n",
    "        for value in values:\n",
    "            data_list = []\n",
    "            for key, df in dict_df.items():\n",
    "                if isinstance(df['cycles_interpolated'][value], float) == False:\n",
    "                    nl =  np.array(df['cycles_interpolated'][value])\n",
    "                    try:\n",
    "                        nl = nl.astype('float32')\n",
    "                        print('reduction done')\n",
    "                    except:\n",
    "                        pass\n",
    "                    nl = list(nl)\n",
    "                    data_list = nl + data_list\n",
    "            data_list = [barcode] + data_list\n",
    "\n",
    "            file_path = os.path.join(path_output, f\"cycles_interpolated_{value}.csv\")\n",
    "            with open(file_path, 'a') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(data_list)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        #create a dict containing all the df corresponding to barcode\n",
    "        print(file_name)\n",
    "        file_path = os.path.join(path_input, file_name)\n",
    "        dict_df[file_name] = pd.read_json(file_path)\n",
    "\n",
    "    values = dict_df[file_names[0]].index\n",
    "\n",
    "    if summary:\n",
    "        for value in values:\n",
    "            data_list = []\n",
    "            for key, df in dict_df.items():\n",
    "                if isinstance(df['summary'][value], float) == False:\n",
    "                    data_list = df['summary'][value] + data_list\n",
    "            data_list = [barcode] + data_list\n",
    "\n",
    "            file_path = os.path.join(path_output, f\"summary_{value}.csv\")\n",
    "            with open(file_path, 'a') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(data_list)\n",
    "     \n",
    "    if cycles:\n",
    "        print(values)\n",
    "        for value in values:\n",
    "            data_list = []\n",
    "            for key, df in dict_df.items():\n",
    "                if isinstance(df['cycles_interpolated'][value], float) == False:\n",
    "                    nl = np.array(df['cycles_interpolated'][value])\n",
    "                    try:\n",
    "                        nl = nl.astype('float32')\n",
    "                        print('reduction done')\n",
    "                    except:\n",
    "                        pass\n",
    "                    nl = list(nl)\n",
    "                    data_list = nl + data_list\n",
    "            data_list = [barcode] + data_list\n",
    "\n",
    "            file_path = os.path.join(path_output, f\"cycles_interpolated_{value}.csv\")\n",
    "            with open(file_path, 'a') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(data_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_data(dir_path, details=True, summary=True, cycles=True, cycles_split=False):\n",
    "    '''\n",
    "    Transformation of the JSON files (one file per battery) into\n",
    "    csv with concatenate data for a given measurement\n",
    "    '''\n",
    "    #path of the files when doing the transformation\n",
    "    initial_data_path = os.path.join(dir_path, \"..\", \"..\", \"raw_data\", \"initial_data\")\n",
    "    initial_data_path = os.path.normpath(initial_data_path)\n",
    "    \n",
    "    transformed_data_path = os.path.join(dir_path, \"..\", \"..\", \"raw_data\", \"transformed_data\")\n",
    "    transformed_data_path = os.path.normpath(transformed_data_path)\n",
    "    \n",
    "    file_droped = 0\n",
    "    file_concat = 0\n",
    "    # files names to transform\n",
    "    onlyfiles = [f for f in os.listdir(initial_data_path) if f[-1] != 'r']\n",
    "    files_dict = {}\n",
    "\n",
    "    # definition of the shape of the 1st file\n",
    "    first_file_path = os.path.join(initial_data_path, onlyfiles[0])\n",
    "    df = pd.read_json(first_file_path)\n",
    "    indexes_list = df.index\n",
    "    column_list = df.columns\n",
    "    \n",
    "    initialize_files(df, transformed_data_path, details, summary, cycles, cycles_split)\n",
    "    i = 0\n",
    "    if details:\n",
    "        for file_name in onlyfiles:\n",
    "            '''\n",
    "            reading each JSON file, and spread data into multiple csv files\n",
    "            data dispatching is done by add_line method\n",
    "            '''\n",
    "            file_path = os.path.join(initial_data_path, file_name)\n",
    "            df = pd.read_json(file_path)\n",
    "            valide_shape(df,indexes_list,column_list,one_val_cols_list,NaN_cols_list)\n",
    "\n",
    "            bc = df['barcode'].iloc[0]\n",
    "\n",
    "            if bc.upper() in barcode_to_drop:\n",
    "                file_droped += 1\n",
    "                print(f\"{bc} has been dropped\")\n",
    "            else:\n",
    "                add_lines_details(df,transformed_data_path)\n",
    "                if bc in files_dict.keys():\n",
    "                    files_dict[bc].append(file_name)\n",
    "                    file_concat += 1\n",
    "                else:\n",
    "                    files_dict[bc] = [file_name]\n",
    "                    i += 1\n",
    "\n",
    "            print(f\"file {i} read a 1st time and added to the dict\")\n",
    "\n",
    "    print(f\"{i} files have been added in the dictionnary\")\n",
    "    print(file_droped)\n",
    "    print(file_concat)\n",
    "    print(len(files_dict))\n",
    "    print(files_dict)\n",
    "\n",
    "    i=0\n",
    "    for barcode, file_names in files_dict.items():\n",
    "        add_lines_data(barcode, file_names,initial_data_path,transformed_data_path)\n",
    "        print(f\"Barcodes {i} read and the data has been added to the csv files\")\n",
    "        i+=1\n",
    "\n",
    "\n",
    "#transform_data(os.path.dirname(__file__))\n",
    "transform_data(\".\", details=False, summary=False, cycles=False, cycles_split=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
